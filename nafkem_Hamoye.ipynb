{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nafkem Hamoye.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/musa1993/B/blob/master/nafkem_Hamoye.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8RPbW1_Ns6_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY2HYw0DEXQ8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeGVJu2BE7LO"
      },
      "outputs": [],
      "source": [
        "train_classes= pd.read_csv('/content/train_v2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ExQEV_JLV4zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Wp_BRbKQg4"
      },
      "outputs": [],
      "source": [
        "train_classes.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKsjpvA7KdtQ"
      },
      "outputs": [],
      "source": [
        "train_classes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X27QQl3bKoWL"
      },
      "outputs": [],
      "source": [
        "train_classes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh-6cohwK0cP"
      },
      "outputs": [],
      "source": [
        "train_classes.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED9FBcIGK8jc"
      },
      "outputs": [],
      "source": [
        "train_classes.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u67ZfqScLbtD"
      },
      "outputs": [],
      "source": [
        "#defining a function to add unique labels to a set\n",
        "labels = set ()\n",
        "def splitting_tags(tags):\n",
        "  for tag in tags.split():\n",
        "      labels.add(tag)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIN3m-jgMLFX"
      },
      "outputs": [],
      "source": [
        "#redefine the train_classes;create a copy to avoid overwritting the existing one.\n",
        "train_classes1=train_classes.copy()\n",
        "\n",
        "train_classes1['tags'].apply(splitting_tags)\n",
        "labels = list(labels)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUcfclxrMduc"
      },
      "outputs": [],
      "source": [
        "train_classes1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnSaSy_pMLVy"
      },
      "outputs": [],
      "source": [
        "#import tensorflow libraies for training the dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,BatchNormalization,Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Dropout,Flatten\n",
        "from tensorflow.keras.optimizers import Adam,SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,TensorBoard\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjVWXk-aMLpM"
      },
      "outputs": [],
      "source": [
        "#assert that the length of the dataframe is the same as the shaoe\n",
        "assert len(train_classes1['image_name'].unique())==train_classes1.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgXwzJeSNU15"
      },
      "outputs": [],
      "source": [
        "#adding .jpg extension to the column image_name so as to have same name format as the image files\n",
        "train_classes1['image_name'].apply(lambda x:'{}.jpg'.format(x))\n",
        "train_classes1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUQXOj2XSRDQ"
      },
      "outputs": [],
      "source": [
        "##One hot encoding is performed on the labels in train classes\n",
        "for tag in labels:\n",
        "    train_classes1[tag] = train_classes1['tags'].apply(lambda x: 1 if tag in x.split() else 0)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## adding .jpg extension to the column image_name so as to have same name format as the image files\n",
        "train_classes1['image_name'] = train_classes1['image_name'].apply(lambda x: '{}.jpg'.format(x))\n",
        "train_classes1.head()"
      ],
      "metadata": {
        "id": "1ImtxngHGi1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt1Hi0u3NUh7"
      },
      "outputs": [],
      "source": [
        "#define a function for accuracy for multi_label classification\n",
        "def multi_label_acc(y_true, y_pred, epsilon = 1e-4):\n",
        "    \n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.cast(tf.greater(tf.cast(y_pred, tf.float32), tf.constant(0.5)), tf.float32)\n",
        "    \n",
        "    tp = tf.reduce_sum(y_true * y_pred, axis = 1)\n",
        "    fp = tf.reduce_sum(y_pred, axis = 1) - tp\n",
        "    fn = tf.reduce_sum(y_true, axis = 1) - tp\n",
        "    \n",
        "    y_true = tf.cast(y_true, tf.bool)\n",
        "    y_pred = tf.cast(y_pred, tf.bool)\n",
        "        \n",
        "    tn = tf.reduce_sum(tf.cast(tf.logical_not(y_true), tf.float32) * tf.cast(tf.logical_not(y_pred), tf.float32), \n",
        "                       axis = 1)\n",
        "    return (tp+tn)/(tp+tn+fp+fn+epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#modelcheckpoint is set to monitor the model using validation fbeta score and save the best only\n",
        "save_best_check_point = ModelCheckpoint(filepath = 'best_model.hdf5', monitor = 'val_fbeta', mode = 'max',save_best_only = True, save_weights_only = True)"
      ],
      "metadata": {
        "id": "uuZnSdAwHX-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing imagedatagenerator with a validation split of 0.2\n",
        "train_image_gen = ImageDataGenerator(rescale = 1/255, validation_split = 0.2)\n"
      ],
      "metadata": {
        "id": "OMBiYt81H0Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = list(train_classes1. columns [2:])"
      ],
      "metadata": {
        "id": "NF6DPpZbMoRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns"
      ],
      "metadata": {
        "id": "5kCAOSizM-2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  model = Sequential()\n",
        "  model.add(BatchNormalization(input_shape=(128, 128,3)))\n",
        "  model.add(Conv2D(32, kernel_size=(3,3),padding='same', activation='relu'))\n",
        "  model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "  model.add(Conv2D(64,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "  model.add(Conv2D(128,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "  model.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "  model.add(Conv2D(256,kernel_size=(3,3),padding='same',activation='relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "  model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(17, activation='sigmoid'))\n",
        "\n",
        "  opt = Adam(ir=1e-4)\n",
        "\n",
        "  model.compile(loss='niinary_crossentropy', optimizer=opt, metrics=[multi_labelacc, fbeta])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "w1m3JspOh1K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up step size for training and validation image data\n",
        "step_train_size = int(np.ceil(train_generator.samples / train_generator.batch_size))\n",
        "step_val_size = int(np.ceil(val_generator.samples / val_generator.batch_size))\n"
      ],
      "metadata": {
        "id": "OIAvgW-rH-uS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initializing the model\n",
        "model1 = build_model()"
      ],
      "metadata": {
        "id": "GViT-35WJnMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this shows the summary of the model, simply put as the model architecture\n",
        "model1.summary()"
      ],
      "metadata": {
        "id": "5DO3LrGKW2BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting our model using the parameters already defined \n",
        "model1.fit(x = train_generator, steps_per_epoch = step_train_size, validation_data = val_generator, validation_steps = step_val_size,epochs = 25, \n",
        "callbacks = [save_best_check_point])"
      ],
      "metadata": {
        "id": "RMU0JGZ4XIKP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}